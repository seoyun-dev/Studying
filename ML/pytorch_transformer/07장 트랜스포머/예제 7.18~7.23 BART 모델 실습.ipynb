{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5809b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/homebrew/lib/python3.11/site-packages (1.4.0)\n",
      "Collecting datasets>=2.0.0 (from evaluate)\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (1.26.1)\n",
      "Collecting dill (from evaluate)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (2.30.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (4.66.2)\n",
      "Collecting xxhash (from evaluate)\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/homebrew/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (0.21.0)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from evaluate) (23.1)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: nltk in /opt/homebrew/lib/python3.11/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/homebrew/lib/python3.11/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.12.0)\n",
      "Collecting pyarrow>=12.0.0 (from datasets>=2.0.0->evaluate)\n",
      "  Downloading pyarrow-15.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets>=2.0.0->evaluate)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting aiohttp (from datasets>=2.0.0->evaluate)\n",
      "  Downloading aiohttp-3.9.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.11/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.11/site-packages (from nltk->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.11/site-packages (from nltk->rouge_score) (2023.12.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->evaluate) (2023.3)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Downloading multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Downloading yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiohttp-3.9.3-cp311-cp311-macosx_11_0_arm64.whl (387 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.7/387.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-15.0.2-cp311-cp311-macosx_11_0_arm64.whl (24.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl (53 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl (81 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=bead65888cf2fc5dfeb1151883d7f41c8800f2aa3b8fca95f5017831c39b8bf3\n",
      "  Stored in directory: /Users/seoyun/Library/Caches/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, frozenlist, dill, yarl, rouge_score, responses, multiprocess, aiosignal, aiohttp, datasets, evaluate\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 datasets-2.18.0 dill-0.3.8 evaluate-0.4.1 frozenlist-1.4.1 multidict-6.0.5 multiprocess-0.70.16 pyarrow-15.0.2 pyarrow-hotfix-0.6 responses-0.18.0 rouge_score-0.1.2 xxhash-3.4.1 yarl-1.9.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate rouge_score absl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a583dd-df09-42a4-bed9-cbe3270fd368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646ff639d102467085b3d6199742f367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.54M/1.54M [00:02<00:00, 643kB/s]\n",
      "Downloading data: 100%|██████████| 31.7M/31.7M [00:12<00:00, 2.60MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e32c1dae974f7fb0550af0cad963c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4881463587e4213ad832b431ca8ce53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/20417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source News : DANANG, Vietnam (Reuters) - Russian President Vladimir Putin said on Saturday he had a normal dialogue with U.S. leader Donald Trump at a summit in Vietnam, and described Trump as civil, well-educated\n",
      "Summarization : Putin says had useful interaction with Trump at Vi\n",
      "Training Data Size : 3000\n",
      "Validation Data Size : 1000\n",
      "Testing Data Size : 1000\n"
     ]
    }
   ],
   "source": [
    "###### 뉴스 요약 데이터세트 불러오기\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 뉴스 본문과 뉴스 요약 텍스트 받아오기\n",
    "news             = load_dataset(\"argilla/news-summary\", split=\"test\")\n",
    "df               = news.to_pandas().sample(5000, random_state=42)[[\"text\", \"prediction\"]]\n",
    "df[\"prediction\"] = df[\"prediction\"].map(lambda x: x[0][\"text\"])\n",
    "# 6:2:2 비율로 학습, 검증 및 테스트 데이터 분리\n",
    "train, valid, test = np.split(\n",
    "    df.sample(frac=1, random_state=42), [int(0.6 * len(df)), int(0.8 * len(df))]\n",
    ")\n",
    "\n",
    "print(f\"Source News : {train.text.iloc[0][:200]}\")\n",
    "print(f\"Summarization : {train.prediction.iloc[0][:50]}\")\n",
    "print(f\"Training Data Size : {len(train)}\")\n",
    "print(f\"Validation Data Size : {len(valid)}\")\n",
    "print(f\"Testing Data Size : {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61a66d8a-bb12-4b74-9ea7-529192ee562e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a46cabe7c34166be76c80a31f8de51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299cce09ae8e415a89cd4c25b870272a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a1a494129d40ff83d85e96d9f5c4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efeb186be854a58bf6993a50e8455ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([   0,  495, 1889,  ...,    1,    1,    1]), tensor([1, 1, 1,  ..., 0, 0, 0]), tensor([    0, 35891,   161,    56,  5616, 10405,    19,   140,    23,  5490,\n",
      "         3564,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]))\n"
     ]
    }
   ],
   "source": [
    "###### BART 입력 텐서 생성\n",
    "import torch\n",
    "from transformers import BartTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "## 배치를 토큰화하고, 패딩, 절사(truncation), 반환 형식을 설정하는 함수\n",
    "def make_dataset(data, tokenizer, device): \n",
    "    tokenized = tokenizer(\n",
    "        text           = data.text.tolist(),\n",
    "        padding        = \"longest\",\n",
    "        truncation     = True,\n",
    "        return_tensors = \"pt\"\n",
    "    )\n",
    "    labels         = []\n",
    "    input_ids      = tokenized[\"input_ids\"].to(device)\n",
    "    attention_mask = tokenized[\"attention_mask\"].to(device) #실제토큰1, 패딩토큰0\n",
    "    for target in data.prediction: \n",
    "        labels.append(tokenizer.encode(target, return_tensors=\"pt\").squeeze())\n",
    "        # 패딩값은 -100. 교차 엔트로피 같은 손실함수에서 패딩된 토큰 무시하게 하기 위함.\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100).to(device)\n",
    "    return TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "\n",
    "\n",
    "def get_datalodader(dataset, sampler, batch_size): \n",
    "    data_sampler = sampler(dataset)\n",
    "    dataloader   = DataLoader(dataset, sampler=data_sampler, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "epochs     = 5\n",
    "batch_size = 8\n",
    "device     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# 사전 학습된 모델 불러오기\n",
    "tokenizer  = BartTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"facebook/bart-base\"\n",
    ")\n",
    "\n",
    "train_dataset    = make_dataset(train, tokenizer, device)\n",
    "train_dataloader = get_datalodader(train_dataset, RandomSampler, batch_size)\n",
    "\n",
    "valid_dataset    = make_dataset(valid, tokenizer, device)\n",
    "valid_dataloader = get_datalodader(valid_dataset, SequentialSampler, batch_size)\n",
    "\n",
    "test_dataset    = make_dataset(test, tokenizer, device)\n",
    "test_dataloader = get_datalodader(test_dataset, SequentialSampler, batch_size)\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b5a022-ff45-4824-846e-5b1193862b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f30e67f2a134cb8b73b0a0f4b7cdf6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### BART 모델 선언\n",
    "from torch import optim\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "# BART 조건부생성 클래스: BART 모델의 변형 중 하나로 조건부 생성 작업에 특화됨\n",
    "# 예를 들어 문장 요약, 기계 번역, 질의 응답 등\n",
    "model = BartForConditionalGeneration.from_pretrained(\n",
    "    # 빠른 학습을 위해 12개 계층아닌 6개의 인코더 디코더 계층 사용하는 모델 사용\n",
    "    pretrained_model_name_or_path=\"facebook/bart-base\"\n",
    ").to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b1e482e-5eb8-416f-a228-56788c328440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "└ shared\n",
      "└ encoder\n",
      "│  └ embed_tokens\n",
      "│  └ embed_positions\n",
      "│  └ layers\n",
      "│  │  └ 0\n",
      "│  │  └ 1\n",
      "│  │  └ 2\n",
      "│  │  └ 3\n",
      "│  │  └ 4\n",
      "│  │  └ 5\n",
      "│  └ layernorm_embedding\n",
      "└ decoder\n",
      "│  └ embed_tokens\n",
      "│  └ embed_positions\n",
      "│  └ layers\n",
      "│  │  └ 0\n",
      "│  │  └ 1\n",
      "│  │  └ 2\n",
      "│  │  └ 3\n",
      "│  │  └ 4\n",
      "│  │  └ 5\n",
      "│  └ layernorm_embedding\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for main_name, main_module in model.named_children():\n",
    "    print(main_name)\n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"└\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"│  └\", ssub_name)\n",
    "            for sssub_name, sssub_module in ssub_module.named_children():\n",
    "                print(\"│  │  └\", sssub_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c78fe2f",
   "metadata": {},
   "source": [
    "- shared:인코더와 디코더가 공유하는 임베딩 계층 (동일한 임베딩 행렬의 공유)\n",
    "- 6계층 인코더 / 디코더\n",
    "    - layernorm_embedding: 인코더와 디코더에서 각 토큰의 임베딩에 적용되는 계층 정규화\n",
    "        임베딩 벡터의 마지막 차원에 대해 정규화를 수행해 학습을 안정화함\n",
    "- lm_head: 선형 임베딩 및 언어모델\n",
    "    마지막 디코더 계층의 출력값은 출력 크기가 단어사전의 크기인 완전 연결 계층을 통과해 언어모델 형성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f854be0d",
   "metadata": {},
   "source": [
    "![](../루지점수.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ad80cc-c0c2-4b03-9ab1-8fa50898ca62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d242851527ed40f29437604adb5a188c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.1614 Val Loss: 1.8398 Val Rouge 0.2524\n",
      "Saved the model weights\n",
      "Epoch 2: Train Loss: 1.6009 Val Loss: 1.8594 Val Rouge 0.2562\n",
      "Epoch 3: Train Loss: 1.2334 Val Loss: 1.9818 Val Rouge 0.2462\n",
      "Epoch 4: Train Loss: 0.9511 Val Loss: 2.0744 Val Rouge 0.2530\n",
      "Epoch 5: Train Loss: 0.7219 Val Loss: 2.2792 Val Rouge 0.2470\n"
     ]
    }
   ],
   "source": [
    "###### BART 모델 학습 및 평가\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "###### BART 평가 방법: 루지 점수 (생성 요약문과 정답 요약문 얼마나 유사한지 평가하기 위해 N-gram 정밀도와 재현율 이용)\n",
    "\n",
    "### 텍스트 요약 작업에서 예측 요약문과 정답 요약문 사이의 루지 점수 계산 함수\n",
    "def calc_rouge(preds, labels):\n",
    "    # preds: 예측한 요약의 토큰 인덱스를 담은 2차원 배열\n",
    "    # argmax를 통해 각 토큰에 대해 가장 높은 확률을 가진 인덱스를 선택하여 1차원 배열로 변경\n",
    "    preds = preds.argmax(axis=-1)\n",
    "    # Labels: 정답 요약문 (레이블이 -100이면 패딩 토큰 인덱스로 변경)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # 특수 토큰을 제외하고 토큰 인덱스를 실제 텍스트로 변환\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # 루지2는 0-1사이 값, 1에 가까울수록 높은 성능\n",
    "    rouge2 = rouge_score.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels\n",
    "    )\n",
    "    return rouge2[\"rouge2\"]\n",
    "\n",
    "def train(model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for input_ids, attention_mask, labels in dataloader:\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    return train_loss\n",
    "\n",
    "def evaluation(model, dataloader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss, val_rouge = 0.0, 0.0\n",
    "\n",
    "        for input_ids, attention_mask, labels in dataloader:\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = labels.to(\"cpu\").numpy()\n",
    "            rouge = calc_rouge(logits, label_ids)\n",
    "            \n",
    "            val_loss += loss\n",
    "            val_rouge += rouge\n",
    "\n",
    "    val_loss = val_loss / len(dataloader)\n",
    "    val_rouge = val_rouge / len(dataloader)\n",
    "    return val_loss, val_rouge\n",
    "\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\", tokenizer=tokenizer)\n",
    "best_loss = 10000\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, optimizer, train_dataloader)\n",
    "    val_loss, val_accuracy = evaluation(model, valid_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Val Rouge {val_accuracy:.4f}\")\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"../models/BartForConditionalGeneration.pt\")\n",
    "        print(\"Saved the model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bba9f3e-f939-4e77-b533-512c84921577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 1.8006\n",
      "Test ROUGE-2 Score : 0.2608\n"
     ]
    }
   ],
   "source": [
    "###### BART 모델 평가\n",
    "model = BartForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"facebook/bart-base\"\n",
    ").to(device)\n",
    "# 선정된 가장 우수한 모델로 문장 요약 평가\n",
    "model.load_state_dict(torch.load(\"../models/BartForConditionalGeneration.pt\"))\n",
    "\n",
    "test_loss, test_rouge_score = evaluation(model, test_dataloader)\n",
    "print(f\"Test Loss : {test_loss:.4f}\")\n",
    "print(f\"Test ROUGE-2 Score : {test_rouge_score:.4f}\")   # 0~1\n",
    "# 학습 시 매우 작은 크기의 데이터로 샘플링해 학습 수행 고려 -> 중간 수준의 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c52711ca-faaa-489b-8f2d-4cf0780a2228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답 요약문 : Clinton leads Trump by 4 points in Washington Post: ABC News poll\n",
      "모델 요약문 : Clinton leads Trump by 4 points in four-war race: Washington Post\n",
      "\n",
      "정답 요약문 : Democrats question independence of Trump Supreme Court nominee\n",
      "모델 요약문 : U.S. senators question whether Gorsuch is independent as Supreme Court nominee\n",
      "\n",
      "정답 요약문 : In push for Yemen aid, U.S. warned Saudis of threats in Congress\n",
      "모델 요약문 : U.S. warns Saudi Arabia about humanitarian conditions in Yemen\n",
      "\n",
      "정답 요약문 : Romanian ruling party leader investigated over 'criminal group'\n",
      "모델 요약문 : Romanian prosecutors arrest leader of ruling party on graft charges\n",
      "\n",
      "정답 요약문 : Billionaire environmental activist Tom Steyer endorses Clinton\n",
      "모델 요약문 : Environmental activist Steyer backs Hillary Clinton for U.S. president\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### 문장 요약문 비교\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "summarizer = pipeline(\n",
    "    task       = \"summarization\",\n",
    "    model      = model,\n",
    "    tokenizer  = tokenizer,\n",
    "    max_length = 54,\n",
    "    device     = \"cpu\"\n",
    ")\n",
    "\n",
    "for index in range(5): \n",
    "    news_text               = test.text.iloc[index]\n",
    "    summarization           = test.prediction.iloc[index]\n",
    "    predicted_summarization = summarizer(news_text)[0][\"summary_text\"]\n",
    "    print(f\"정답 요약문 : {summarization}\")\n",
    "    print(f\"모델 요약문 : {predicted_summarization}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
