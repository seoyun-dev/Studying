{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db76f6d-3094-4ca7-bc0b-e9254b4ea8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source News : summarize: DANANG, Vietnam (Reuters) - Russian President Vladimir Putin said on Saturday he had a normal dialogue with U.S. leader Donald Trump at a summit in Vietnam, and described Trump as civil, we\n",
      "Summarization : Putin says had useful interaction with Trump at Vi\n",
      "Training Data Size : 3000\n",
      "Validation Data Size : 1000\n",
      "Testing Data Size : 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "########## 뉴스 요약 데이터세트 불러오기 (뉴스 본문과 요약된 뉴스)\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "news             = load_dataset(\"argilla/news-summary\", split=\"test\")\n",
    "df               = news.to_pandas().sample(5000, random_state=42)[[\"text\", \"prediction\"]]\n",
    "# summarize:를 붙여 요약 작업이라는 정보를 모델에 전달\n",
    "df[\"text\"]       = \"summarize: \" + df[\"text\"]\n",
    "df[\"prediction\"] = df[\"prediction\"].map(lambda x: x[0][\"text\"])\n",
    "train, valid, test = np.split(\n",
    "    df.sample(frac=1, random_state=42), [int(0.6 * len(df)), int(0.8 * len(df))]\n",
    ")\n",
    "\n",
    "print(f\"Source News : {train.text.iloc[0][:200]}\")\n",
    "print(f\"Summarization : {train.prediction.iloc[0][:50]}\")\n",
    "print(f\"Training Data Size : {len(train)}\")\n",
    "print(f\"Validation Data Size : {len(valid)}\")\n",
    "print(f\"Testing Data Size : {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f83bba-91e9-4dc6-bc96-d4764f8ec065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f8429a8d5445c0aba578d2b045e415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810ab0e2ff98482286259f8cc9d5716c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a792d3589f041648884c93c14fbc24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[21603,    10,   301,  ...,  1515,  4356,     1],\n",
      "        [21603,    10,  5292,  ...,   116,  3124,     1],\n",
      "        [21603,    10,    71,  ...,    16,     3,     1],\n",
      "        ...,\n",
      "        [21603,    10,   549,  ...,    12,  4514,     1],\n",
      "        [21603,    10, 11175,  ...,     3,     9,     1],\n",
      "        [21603,    10,     3,  ...,   932,    65,     1]], device='mps:0'), tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='mps:0'), tensor([[ 1270,  7713,     6,  ...,     0,     0,     0],\n",
      "        [ 5245,  3654,    38,  ...,     0,     0,     0],\n",
      "        [  749,    18,   439,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  445,   956,  5923,  ...,     0,     0,     0],\n",
      "        [  412,     5,   134,  ...,     0,     0,     0],\n",
      "        [13857,  7262,  4947,  ...,     0,     0,     0]], device='mps:0'), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='mps:0')]\n",
      "▁summarize\n",
      ":\n"
     ]
    }
   ],
   "source": [
    "########## 뉴스 요약 데이터세트 전처리\n",
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def make_dataset(data, tokenizer, device): \n",
    "    source = tokenizer(\n",
    "        text              = data.text.tolist(),\n",
    "        padding           = \"max_length\", \n",
    "        max_length        = 128,\n",
    "        pad_to_max_length = True, # max_length 기준보다 짧으면 패딩\n",
    "        truncation        = True, # max_length 기준보다 길면 자르기\n",
    "        return_tensors    = \"pt\"\n",
    "    )\n",
    "\n",
    "    target = tokenizer(\n",
    "        text              = data.prediction.tolist(),\n",
    "        padding           = \"max_length\",\n",
    "        max_length        = 128,\n",
    "        pad_to_max_length = True,\n",
    "        truncation        = True,\n",
    "        return_tensors    = \"pt\"\n",
    "    )\n",
    "    \n",
    "    source_ids  = source[\"input_ids\"].squeeze().to(device)\n",
    "    source_mask = source[\"attention_mask\"].squeeze().to(device)\n",
    "    target_ids  = target[\"input_ids\"].squeeze().to(device)\n",
    "    target_mask = target[\"attention_mask\"].squeeze().to(device)\n",
    "    return TensorDataset(source_ids, source_mask, target_ids, target_mask)\n",
    "\n",
    "def get_datalodader(dataset, sampler, batch_size): \n",
    "    data_sampler = sampler(dataset)\n",
    "    dataloader   = DataLoader(dataset, sampler=data_sampler, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "epochs     = 5\n",
    "batch_size = 8\n",
    "device     = \"mps\" if torch.backends.mps.is_available() and torch.backends.mps.is_built() else \"cpu\"\n",
    "# T5 모델의 기본 버전 불러와 전처리 수행\n",
    "tokenizer  = T5Tokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"t5-small\"\n",
    ")\n",
    "\n",
    "train_dataset    = make_dataset(train, tokenizer, device)\n",
    "train_dataloader = get_datalodader(train_dataset, RandomSampler, batch_size)\n",
    "\n",
    "valid_dataset    = make_dataset(valid, tokenizer, device)\n",
    "valid_dataloader = get_datalodader(valid_dataset, SequentialSampler, batch_size)\n",
    "\n",
    "test_dataset    = make_dataset(test, tokenizer, device)\n",
    "test_dataloader = get_datalodader(test_dataset, SequentialSampler, batch_size)\n",
    "\n",
    "print(next(iter(train_dataloader)))\n",
    "# 21603, 10 = summarize:\n",
    "print(tokenizer.convert_ids_to_tokens(21603))\n",
    "print(tokenizer.convert_ids_to_tokens(10))\n",
    "# input tokens, input attention mask, output token, output attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f4c2969-7300-4306-aef1-6091328e2310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f4c10f5b1f40bb8ac8d3be2540e98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f80b56dfe8a4af8917866d1bbc55a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2bcfb115e84604be0c70667477059a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########## T5 모델 선언\n",
    "from torch import optim\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"t5-small\",\n",
    ").to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e5347be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared\n",
      "encoder\n",
      "└ embed_tokens\n",
      "└ block\n",
      "│  └ 0\n",
      "│  │  └ layer\n",
      "│  └ 1\n",
      "│  │  └ layer\n",
      "│  └ 2\n",
      "│  │  └ layer\n",
      "│  └ 3\n",
      "│  │  └ layer\n",
      "│  └ 4\n",
      "│  │  └ layer\n",
      "│  └ 5\n",
      "│  │  └ layer\n",
      "└ final_layer_norm\n",
      "└ dropout\n",
      "decoder\n",
      "└ embed_tokens\n",
      "└ block\n",
      "│  └ 0\n",
      "│  │  └ layer\n",
      "│  └ 1\n",
      "│  │  └ layer\n",
      "│  └ 2\n",
      "│  │  └ layer\n",
      "│  └ 3\n",
      "│  │  └ layer\n",
      "│  └ 4\n",
      "│  │  └ layer\n",
      "│  └ 5\n",
      "│  │  └ layer\n",
      "└ final_layer_norm\n",
      "└ dropout\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for main_name, main_module in model.named_children():\n",
    "    print(main_name)\n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"└\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"│  └\", ssub_name)\n",
    "            for sssub_name, sssub_module in ssub_module.named_children():\n",
    "                print(\"│  │  └\", sssub_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee89329",
   "metadata": {},
   "source": [
    "shared: 인코더와 디코더 함수에 상요되는 토큰 임베딩. 서로 공유."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e81b0e5-327c-4604-bb8b-fff9786c5e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.8140 Val Loss: 2.5673\n",
      "Saved the model weights\n",
      "Epoch 2: Train Loss: 2.7508 Val Loss: 2.5278\n",
      "Saved the model weights\n",
      "Epoch 3: Train Loss: 2.6893 Val Loss: 2.4946\n",
      "Saved the model weights\n",
      "Epoch 4: Train Loss: 2.6484 Val Loss: 2.4676\n",
      "Saved the model weights\n",
      "Epoch 5: Train Loss: 2.6057 Val Loss: 2.4471\n",
      "Saved the model weights\n"
     ]
    }
   ],
   "source": [
    "########## T5 모델 학습 및 평가\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def calc_accuracy(preds, labels): \n",
    "    pred_flat   = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def train(model, optimizer, dataloader): \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    ######### 변경부분\n",
    "    # 토큰인덱스, 어텐션마스크, 디코더토큰인덱스, 라벨로 모델을 학습\n",
    "    for source_ids, source_mask, target_ids, target_mask in dataloader: \n",
    "        # 마지막 토큰을 제외한 나머지 토큰\n",
    "        decoder_input_ids                                   = target_ids[:, :-1].contiguous()\n",
    "        # 다음 시점 예측하도록 첫 번째 토큰 제외한 토큰\n",
    "        labels                                              = target_ids[:, 1:].clone().detach()\n",
    "        labels[target_ids[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids         = source_ids,\n",
    "            attention_mask    = source_mask,\n",
    "            decoder_input_ids = decoder_input_ids,\n",
    "            labels            = labels,\n",
    "        )\n",
    "    ######### \n",
    "        loss        = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def evaluation(model, dataloader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        ######### 변경부분\n",
    "        for source_ids, source_mask, target_ids, target_mask in dataloader:\n",
    "            decoder_input_ids = target_ids[:, :-1].contiguous()\n",
    "            labels            = target_ids[:, 1:].clone().detach()\n",
    "            labels[target_ids[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids         = source_ids,\n",
    "                attention_mask    = source_mask,\n",
    "                decoder_input_ids = decoder_input_ids,\n",
    "                labels            = labels,\n",
    "            )\n",
    "        #########\n",
    "\n",
    "            loss      = outputs.loss\n",
    "            val_loss += loss\n",
    "\n",
    "    val_loss = val_loss / len(dataloader)\n",
    "    ######### 변경부분\n",
    "    return val_loss\n",
    "    ######### \n",
    "\n",
    "\n",
    "best_loss = 10000\n",
    "for epoch in range(epochs): \n",
    "    train_loss = train(model, optimizer, train_dataloader)\n",
    "    ######### 변경부분\n",
    "    val_loss   = evaluation(model, valid_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}\")\n",
    "    ######### \n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"../models/T5ForConditionalGeneration.pt\")\n",
    "        print(\"Saved the model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4422e000-d322-4152-a47a-037c35008eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/beam_search.py:377: UserWarning: MPS: no support for int64 for min_max, downcasting to a smaller data type (int32/float32). Native support for int64 has been added in macOS 13.3. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:621.)\n",
      "  sent_lengths_max = sent_lengths.max().item() + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Headline Text: Clinton leads Trump by 4 percentage points in four-war race for Nov. 8 election\n",
      "Actual Headline Text   : Clinton leads Trump by 4 points in Washington Post: ABC News poll\n",
      "Generated Headline Text: U.S. senators sharpen line of attack against Gorsuch's nomination to Supreme Court\n",
      "Actual Headline Text   : Democrats question independence of Trump Supreme Court nominee\n",
      "Generated Headline Text: U.S. warns Saudi Arabia over Yemen's humanitarian situation could constrain U.S. aid.\n",
      "Actual Headline Text   : In push for Yemen aid, U.S. warned Saudis of threats in Congress\n",
      "Generated Headline Text: Romanian anti-corruption prosecutors open investigation into Liviu Dragnea on suspicion of forming criminal group to siphon off cash from state projects\n",
      "Actual Headline Text   : Romanian ruling party leader investigated over 'criminal group'\n",
      "Generated Headline Text: environmental activist endorsed Hillary Clinton for U.S. president\n",
      "Actual Headline Text   : Billionaire environmental activist Tom Steyer endorses Clinton\n",
      "Generated Headline Text: the 74-year-old grandmother delivers news of Pyongyang nuclear test with her usual gusto.\n",
      "Actual Headline Text   : Voice of triumph or doom: North Korean presenter back in limelight for nuclear test\n",
      "Generated Headline Text: Delson Guarate, Yon Goicoechea among nearly 400 jailed anti-Maduro activists in jail for more than a year\n",
      "Actual Headline Text   : Venezuela frees two anti-Maduro activists; scores still jailed\n",
      "Generated Headline Text: House Majority Leader McCarthy says he still troubled by Clinton email server\n",
      "Actual Headline Text   : House No. 2 Republican says still questions Clinton's judgment in email matter\n"
     ]
    }
   ],
   "source": [
    "########## T5 생성 모델 테스트\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for source_ids, source_mask, target_ids, target_mask in test_dataloader:\n",
    "        # generate: 입력 시퀀스에 대한 요약문(출력 시퀀스)을 생성\n",
    "        generated_ids = model.generate(\n",
    "            input_ids          = source_ids,\n",
    "            attention_mask     = source_mask,\n",
    "            max_length         = 128,   # 요약문 최대길이\n",
    "            num_beams          = 3,     # 빔 크기\n",
    "            repetition_penalty = 2.5,   # 중복 토큰 생성을 제어 (1기준.값이 높을수록 중복토큰 생성 억제)\n",
    "            length_penalty     = 1.0,   # 생성 시퀀스 길이에 대한 보상 제어 (1기준.값이 높을수록 더욱 긴 시퀀스 생성)\n",
    "            early_stopping     = True,  # 최대 길이에 도달하기 전 EOS 토큰이 생성되는 경우 중단\n",
    "        )\n",
    "\n",
    "        for generated, target in zip(generated_ids, target_ids): \n",
    "            pred = tokenizer.decode(\n",
    "                generated, skip_special_tokens = True, clean_up_tokenization_spaces = True\n",
    "            )\n",
    "            actual = tokenizer.decode(\n",
    "                target, skip_special_tokens = True, clean_up_tokenization_spaces = True\n",
    "            )\n",
    "            print(\"Generated Headline Text:\", pred) \n",
    "            print(\"Actual Headline Text   :\", actual) \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7eccad",
   "metadata": {},
   "source": [
    "디코더 모델은 다음 단어를 예측하는 과정에서 다수의 후보 단어를 생성한다.  \n",
    "이때 빔서치 알고리즘은 미리 지정한 빔 크기만큼의 후보 단어 시퀀스만을 유지하고, 나머지 후보 시퀀스들은 삭제한다.  \n",
    "이후 다음 단어를 예측하면서 빔 크기에 맞게 후보 시퀀스들을 업데이트하며, 최종적으로 가장 높은 확률을 가진 시퀀스를 선택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b672988",
   "metadata": {},
   "source": [
    "k개의 빔에서 각각 다음 예측값의 확률 분포 중 가장 높은 k개를 자식노드로 만듦 -> k제곱개의 자식 중 상위 k개만 남김 -> eos를 만난 빔이 k개가 될 때까지 위 두 과정을 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c00aa03",
   "metadata": {},
   "source": [
    "![](../빔서치.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1f50a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
