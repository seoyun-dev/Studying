{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc1637c8-d4c0-464d-9f6b-1a8be4b48b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
      "{'T-shirt/top': 0, 'Trouser': 1, 'Pullover': 2, 'Dress': 3, 'Coat': 4, 'Sandal': 5, 'Shirt': 6, 'Sneaker': 7, 'Bag': 8, 'Ankle boot': 9}\n",
      "Training Data Size : 10000\n",
      "Testing Data Size : 1000\n",
      "(<PIL.Image.Image image mode=L size=28x28 at 0x2AF44BD50>, 9) (<PIL.Image.Image image mode=L size=28x28 at 0x2AF44BDD0>, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "######## 10.1 FashinMNIST 다운로드: 데이터 불러오고 서브셋 만듦\n",
    "\n",
    "from itertools import chain # 여러 리스트를 하나의 리스트로 연결\n",
    "from collections import defaultdict # 기본값이 리스트인 딕셔너리 생성\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "def subset_sampler(dataset, classes, max_len):\n",
    "    target_idx = defaultdict(list) # 각 클래스의 인덱스를 저장하는 dict\n",
    "    # dataset.train_labels=각 이미지에 해당하는 클래스(label)(0~9)의 정수 배열\n",
    "    for idx, label in enumerate(dataset.train_labels):\n",
    "        target_idx[int(label)].append(idx)\n",
    "\n",
    "    # 각 클래스별로 max_len만큼의 인덱스를 추출하여 하나의 리스트로 만듦\n",
    "    indices = list(\n",
    "        chain.from_iterable(\n",
    "            [target_idx[idx][:max_len] for idx in range(len(classes))]\n",
    "        )\n",
    "    )\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=True)\n",
    "test_dataset  = datasets.FashionMNIST(root=\"../datasets\", download=True, train=False)\n",
    "\n",
    "# train dataset에 포함된 클래스\n",
    "classes = train_dataset.classes\n",
    "# train dataset의 클래스와 클래스id가 매핑된 값\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "\n",
    "print(classes)\n",
    "print(class_to_idx)\n",
    "\n",
    "# 각 클래스별로 최대 1000개의 샘플을 포함하는 훈련 데이터 서브셋 만듦\n",
    "subset_train_dataset = subset_sampler(\n",
    "    dataset = train_dataset, classes = train_dataset.classes, max_len = 1000\n",
    ")\n",
    "# 각 클래스별로 최대 100개의 샘플을 포함하는 테스트 데이터 서브셋 만듦\n",
    "subset_test_dataset = subset_sampler(\n",
    "    dataset = test_dataset, classes = test_dataset.classes, max_len = 100\n",
    ")\n",
    "\n",
    "print(f\"Training Data Size : {len(subset_train_dataset)}\") # max_len x len(classes) = 1000x10\n",
    "print(f\"Testing Data Size : {len(subset_test_dataset)}\")\n",
    "print(train_dataset[0], subset_train_dataset[0]) #(PIL이미지, class label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43331628-60a7-4640-b547-3badae816264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size : {'height': 224, 'width': 224}\n",
      "mean : [0.5, 0.5, 0.5]\n",
      "std : [0.5, 0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "######## 10.2 이미지 전처리\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "# 사전학습된 ViT 모델 활용해 전처리 진행\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"google/vit-base-patch16-224-in21k\"\n",
    ")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # 모델 학습을 위해 PIL.image > Tensor\n",
    "        transforms.ToTensor(),\n",
    "        # 크기 조정\n",
    "        transforms.Resize(\n",
    "            size=(\n",
    "                image_processor.size[\"height\"],\n",
    "                image_processor.size[\"width\"]\n",
    "            )\n",
    "        ),\n",
    "        # 단일 채널을 복제해 다중 채널 이미지로 변환\n",
    "        transforms.Lambda(\n",
    "            # FashinMNIST의 x는 흑백이미지 [1,H,W]\n",
    "            # 텐서 3개로 복사하여 0번차원 기준으로 cat(연결)\n",
    "            #결과: [3,H,W]\n",
    "            lambda x: torch.cat([x, x, x], 0)\n",
    "        ),\n",
    "        # 정규화\n",
    "        transforms.Normalize(\n",
    "            mean = image_processor.image_mean,\n",
    "            std  = image_processor.image_std\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"size : {image_processor.size}\")\n",
    "print(f\"mean : {image_processor.image_mean}\")\n",
    "print(f\"std : {image_processor.image_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32f9ad8d-a37a-452c-aee8-0562c982bb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values : torch.Size([32, 3, 224, 224])\n",
      "labels : torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "######## 10.3 ViT 모델 구조에 맞는 형태로 변환해데이터로더 적용\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ViT모델은 Tensor 형식이 아닌 딕셔너리 형식의 데이터를 입력으로 받음\n",
    "# ViT 모델의 입력: {\"pixel_values\":pixel_values, \"labels\":labels}\n",
    "\n",
    "def collator(data, transform):\n",
    "    images, labels = zip(*data)\n",
    "    # pixel_values = (배치크기, 채널수, 이미지높이, 이미지너비)\n",
    "    pixel_values = torch.stack([transform(image) for image in images])\n",
    "    # labels = [클래스 색인값]\n",
    "    labels       = torch.tensor([label for label in labels])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    subset_train_dataset, # (PIL이미지, label)\n",
    "    batch_size = 32,\n",
    "    shuffle    = True,\n",
    "    collate_fn = lambda x: collator(x, transform),\n",
    "    drop_last  = True\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    subset_test_dataset,\n",
    "    batch_size = 4,\n",
    "    shuffle    = True,\n",
    "    collate_fn = lambda x: collator(x, transform),\n",
    "    drop_last  = True\n",
    ")\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "# batch = {\"pixel_values\":(배치크기, 채널수, 이미지높이, 이미지너비), \"labels\":배치 크기의 클래스 색인 list}\n",
    "for key, value in batch.items():\n",
    "    print(f\"{key} : {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e821280f-e4e7-4aa0-9328-d95a1e3cc139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "######## 10.4 사전 학습된 ViT 모델\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    # 사전 학습된 모델은 앞의 이미지 프로세서 클래스에서 사용한 모델과 동일\n",
    "    pretrained_model_name_or_path = \"google/vit-base-patch16-224-in21k\",\n",
    "    # 아래의 3개 매개변수를 통해 현재 데이터세트에 적합한 구조로 모델을 미세조정\n",
    "    num_labels                    = len(classes),\n",
    "    id2label                      = {idx: label for label, idx in class_to_idx.items()},\n",
    "    label2id                      = class_to_idx,\n",
    "    ignore_mismatched_sizes       = True\n",
    ")\n",
    "\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1414f6a-c1be-460a-b6ac-40790faafc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTEmbeddings(\n",
      "  (patch_embeddings): ViTPatchEmbeddings(\n",
      "    (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "image shape : torch.Size([32, 3, 224, 224])\n",
      "patch embeddings shape : torch.Size([32, 196, 768])\n",
      "[CLS] + patch embeddings shape : torch.Size([32, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "######## 10.5 패치 임베딩 확인\n",
    "print(model.vit.embeddings)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "# 기존 이미지 (배치사이즈, 채널수, 이미지높이, 이미지너비)\n",
    "print(\"image shape :\", batch[\"pixel_values\"].shape)\n",
    "\n",
    "# 임베딩 후 (배치크기, 패치수(14x14), 임베딩길이)\n",
    "print(\"patch embeddings shape :\",\n",
    "    model.vit.embeddings.patch_embeddings(batch[\"pixel_values\"]).shape\n",
    ")\n",
    "print(\"[CLS] + patch embeddings shape :\",\n",
    "    model.vit.embeddings(batch[\"pixel_values\"]).shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba5725ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.30.2 accelerate==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5b1b460-47aa-4c5f-9086-aec48a725548",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## 10.6 하이퍼파라미터 설정\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# TrainingArguments(학습매개변수클래스): 모델 학습에 필요한 다양한 인자들을 저장하고 관리\n",
    "args = TrainingArguments(\n",
    "    output_dir                  = \"../models/ViT-FashionMNIST\", # 체크포인트 저장경로\n",
    "    save_strategy               = \"epoch\", # 체크포인트 저장간격\n",
    "    evaluation_strategy         = \"epoch\", # 체크포인트 평가간격\n",
    "    learning_rate               = 1e-5,\n",
    "    per_device_train_batch_size = 16,      # 학습 배치 크기\n",
    "    per_device_eval_batch_size  = 16,      # 평가 배치 크기\n",
    "    num_train_epochs            = 3,       # 학습 반복 수\n",
    "    weight_decay                = 0.001,\n",
    "    load_best_model_at_end      = True,\n",
    "    metric_for_best_model       = \"f1\",     # 최상의 모델 선정 기준 평가 방식\n",
    "    logging_dir                 = \"logs\",   # 로그 저장 폴더\n",
    "    logging_steps               = 125,      # 로그 출력 간격\n",
    "    remove_unused_columns       = False,\n",
    "    seed                        = 7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83101c79-51f6-4b23-83ef-082e15bdd980",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## 10.7 매크로 평균 F1 점수\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = evaluate.load(\"f1\")\n",
    "    # predictions: 모든 클래스에 대한 예측값, labels: 실제값\n",
    "    predictions, labels = eval_pred \n",
    "    # 가장 확률이 높은 값을 저장\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # f1 점수 계산\n",
    "    macro_f1    = metric.compute(\n",
    "        # 매크로 평균 F1 점수 방식 사용하므로 average매개변수에 macro 입력\n",
    "        predictions = predictions, references = labels, average = \"macro\"\n",
    "    )\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6253e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## 10.8 ViT 모델 학습 (Colab에서 실행)\n",
    "import torch\n",
    "from transformers import ViTForImageClassification\n",
    "from transformers import Trainer\n",
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "\n",
    "def model_init(classes, class_to_idx):\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        pretrained_model_name_or_path = \"google/vit-base-patch16-224-in21k\",\n",
    "        num_labels                    = len(classes),\n",
    "        id2label                      = {idx: label for label, idx in class_to_idx.items()},\n",
    "        label2id                      = class_to_idx,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Trainer 클래스를 통해 ViT 모델 학습 수행\n",
    "trainer = Trainer(\n",
    "    # model_init      = model_init,\n",
    "    model_init      = lambda x: model_init(classes, class_to_idx),\n",
    "    args            = args,\n",
    "    train_dataset   = subset_train_dataset,\n",
    "    eval_dataset    = subset_test_dataset,\n",
    "    data_collator   = lambda x: collator(x, transform),\n",
    "    compute_metrics = compute_metrics,\n",
    "    tokenizer       = image_processor,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f89b3b4",
   "metadata": {},
   "source": [
    "![](../screenshot/vit-train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a336dc1",
   "metadata": {},
   "source": [
    "최종학습의 손실값은 0.4049이며 F1 점수가 0.9232로 학습이 우수하게 진행된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6072e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "outputs = trainer.predict(subset_test_dataset)\n",
    "print(outputs)\n",
    "\n",
    "y_true = outputs.label_ids\n",
    "y_pred = outputs.predictions.argmax(1)\n",
    "\n",
    "labels  = list(classes)\n",
    "# confusion_matrix 활용해 성능 평가\n",
    "matrix  = confusion_matrix(y_true, y_pred)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=labels)\n",
    "_, ax   = plt.subplots(figsize=(10, 10))\n",
    "display.plot(xticks_rotation=45, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# 혼동 행렬을 통해 오분류된 클래스에 대한 문제점을 파악하고 모델을 개선해 정확도를 향상시킬 수 있음\n",
    "# 주요한 모델 개선 방법: 하이퍼파라미터 조정, 모델그조 변경, 전처리 과정 개선, 데이터 증강 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d27b3c",
   "metadata": {},
   "source": [
    "metrics={'test_loss': 0.43413683772087097, 'test_f1': 0.9232480514680315, 'test_runtime': 13.459, 'test_samples_per_second': 74.3, 'test_steps_per_second': 4.681}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37758060",
   "metadata": {},
   "source": [
    "![](../screenshot/vit-test2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
