{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d63c9c-6298-4615-a205-4b9b5590cc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "def subset_sampler(dataset, classes, max_len):\n",
    "    target_idx = defaultdict(list) # 각 클래스의 인덱스를 저장하는 dict\n",
    "    # dataset.train_labels=각 이미지에 해당하는 클래스(label)(0~9)의 정수 배열\n",
    "    for idx, label in enumerate(dataset.train_labels):\n",
    "        target_idx[int(label)].append(idx)\n",
    "\n",
    "    # 각 클래스별로 max_len만큼의 인덱스를 추출하여 하나의 리스트로 만듦\n",
    "    indices = list(\n",
    "        chain.from_iterable(\n",
    "            [target_idx[idx][:max_len] for idx in range(len(classes))]\n",
    "        )\n",
    "    )\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=True)\n",
    "test_dataset  = datasets.FashionMNIST(root=\"../datasets\", download=True, train=False)\n",
    "\n",
    "# train dataset에 포함된 클래스\n",
    "classes = train_dataset.classes\n",
    "# train dataset의 클래스와 클래스id가 매핑된 값\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "\n",
    "subset_train_dataset = subset_sampler(\n",
    "    dataset = train_dataset, classes = train_dataset.classes, max_len = 1000\n",
    ")\n",
    "subset_test_dataset = subset_sampler(\n",
    "    dataset = test_dataset, classes = test_dataset.classes, max_len = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90cf39e2-867c-4d98-a460-44f502021bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"google/vit-base-patch16-224-in21k\"\n",
    ")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(\n",
    "            size=(\n",
    "                image_processor.size[\"height\"],\n",
    "                image_processor.size[\"width\"]\n",
    "            )\n",
    "        ),\n",
    "        transforms.Lambda(\n",
    "            lambda x: torch.cat([x, x, x], 0)\n",
    "        ),\n",
    "        transforms.Normalize(\n",
    "            mean = image_processor.image_mean,\n",
    "            std  = image_processor.image_std\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb15c67-306b-4ffe-bccd-35bb6fdf2d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collator(data, transform):\n",
    "    images, labels = zip(*data)\n",
    "    pixel_values = torch.stack([transform(image) for image in images])\n",
    "    labels       = torch.tensor([label for label in labels])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    subset_train_dataset,\n",
    "    batch_size = 32,\n",
    "    shuffle    = True,\n",
    "    collate_fn = lambda x: collator(x, transform),\n",
    "    drop_last  = True\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    subset_test_dataset,\n",
    "    batch_size = 4,\n",
    "    shuffle    = True,\n",
    "    collate_fn = lambda x: collator(x, transform),\n",
    "    drop_last  = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b0e9422-7399-4531-b34d-6d84e1b1d890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c25b74fa6c4c77b7bc6007b83066a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/71.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e069376bba5549fc8fcdde1a0c52c589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/113M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swin\n",
      "└ embeddings\n",
      "│  └ patch_embeddings\n",
      "│  │  └ projection Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "│  └ norm\n",
      "│  └ dropout\n",
      "└ encoder\n",
      "│  └ layers\n",
      "│  │  └ 0\n",
      "│  │  └ 1\n",
      "│  │  └ 2\n",
      "│  │  └ 3\n",
      "└ layernorm\n",
      "└ pooler\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "from transformers import SwinForImageClassification\n",
    "\n",
    "\n",
    "model = SwinForImageClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"microsoft/swin-tiny-patch4-window7-224\",\n",
    "    num_labels                    = len(train_dataset.classes),\n",
    "    id2label                      = {idx: label for label, idx in train_dataset.class_to_idx.items()},\n",
    "    label2id                      = train_dataset.class_to_idx,\n",
    "    ignore_mismatched_sizes       = True\n",
    ")\n",
    "\n",
    "for main_name, main_module in model.named_children():\n",
    "    print(main_name)\n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"└\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"│  └\", ssub_name)\n",
    "            for sssub_name, sssub_module in ssub_module.named_children():\n",
    "                if sssub_name == \"projection\":\n",
    "                    print(\"│  │  └\", sssub_name, sssub_module)\n",
    "                else:\n",
    "                    print(\"│  │  └\", sssub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a01960-903b-45f5-9c54-760098f8c211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 차원 : torch.Size([32, 3, 224, 224])\n",
      "모듈: SwinPatchEmbeddings(\n",
      "  (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      ")\n",
      "패치 임베딩 차원 : torch.Size([32, 3136, 96])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(\"이미지 차원 :\", batch[\"pixel_values\"].shape)\n",
    "\n",
    "patch_emb_output, shape = model.swin.embeddings.patch_embeddings(batch[\"pixel_values\"])\n",
    "print(\"모듈:\", model.swin.embeddings.patch_embeddings)\n",
    "print(\"패치 임베딩 차원 :\", patch_emb_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25955635-41b2-4516-8c5f-7b98fddf9edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks\n",
      "└ 0\n",
      "│ └ layernorm_before\n",
      "│ └ attention\n",
      "│ └ drop_path\n",
      "│ └ layernorm_after\n",
      "│ └ intermediate\n",
      "│ └ output\n",
      "└ 1\n",
      "│ └ layernorm_before\n",
      "│ └ attention\n",
      "│ └ drop_path\n",
      "│ └ layernorm_after\n",
      "│ └ intermediate\n",
      "│ └ output\n",
      "downsample\n",
      "└ reduction\n",
      "└ norm\n"
     ]
    }
   ],
   "source": [
    "for main_name, main_module in model.swin.encoder.layers[0].named_children():\n",
    "    print(main_name) \n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"└\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"│ └\", ssub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f270eb2-37a6-49ff-b937-4b1bb9d5496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinLayer(\n",
      "  (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): SwinAttention(\n",
      "    (self): SwinSelfAttention(\n",
      "      (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "      (key): Linear(in_features=96, out_features=96, bias=True)\n",
      "      (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): SwinSelfOutput(\n",
      "      (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (drop_path): SwinDropPath(p=0.1)\n",
      "  (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  (intermediate): SwinIntermediate(\n",
      "    (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): SwinOutput(\n",
      "    (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.swin.encoder.layers[0].blocks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d21bf119-4613-4a1c-94c9-0fb002b1989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패치 임베딩 차원 : torch.Size([32, 3136, 96])\n",
      "W-MSA 결과 차원 : torch.Size([32, 3136, 96])\n",
      "SW-MSA 결과 차원 : torch.Size([32, 3136, 96])\n"
     ]
    }
   ],
   "source": [
    "print(\"패치 임베딩 차원 :\", patch_emb_output.shape)\n",
    "\n",
    "W_MSA  = model.swin.encoder.layers[0].blocks[0]\n",
    "SW_MSA = model.swin.encoder.layers[0].blocks[1]\n",
    "\n",
    "W_MSA_output  = W_MSA(patch_emb_output, W_MSA.input_resolution)[0]\n",
    "SW_MSA_output = SW_MSA(W_MSA_output, SW_MSA.input_resolution)[0]\n",
    "\n",
    "print(\"W-MSA 결과 차원 :\", W_MSA_output.shape)\n",
    "print(\"SW-MSA 결과 차원 :\", SW_MSA_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e0f0972-23dc-4136-a94c-ec18eb6d3acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_merge 모듈 : SwinPatchMerging(\n",
      "  (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "patch_merge 결과 차원 : torch.Size([32, 784, 192])\n"
     ]
    }
   ],
   "source": [
    "patch_merge = model.swin.encoder.layers[0].downsample\n",
    "print(\"patch_merge 모듈 :\", patch_merge)\n",
    "\n",
    "output = patch_merge(SW_MSA_output, patch_merge.input_resolution)\n",
    "print(\"patch_merge 결과 차원 :\", output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
