{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d63c9c-6298-4615-a205-4b9b5590cc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "####### 데이터 불러오고 서브셋 만듦\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "def subset_sampler(dataset, classes, max_len):\n",
    "    target_idx = defaultdict(list) # 각 클래스의 인덱스를 저장하는 dict\n",
    "    # dataset.train_labels=각 이미지에 해당하는 클래스(label)(0~9)의 정수 배열\n",
    "    for idx, label in enumerate(dataset.train_labels):\n",
    "        target_idx[int(label)].append(idx)\n",
    "\n",
    "    # 각 클래스별로 max_len만큼의 인덱스를 추출하여 하나의 리스트로 만듦\n",
    "    indices = list(\n",
    "        chain.from_iterable(\n",
    "            [target_idx[idx][:max_len] for idx in range(len(classes))]\n",
    "        )\n",
    "    )\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=True)\n",
    "test_dataset  = datasets.FashionMNIST(root=\"../datasets\", download=True, train=False)\n",
    "\n",
    "# train dataset에 포함된 클래스\n",
    "classes = train_dataset.classes\n",
    "# train dataset의 클래스와 클래스id가 매핑된 값\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "\n",
    "# 각 클래스별로 최대 1000개의 샘플을 포함하는 훈련 데이터 서브셋 만듦\n",
    "subset_train_dataset = subset_sampler(\n",
    "    dataset = train_dataset, classes = train_dataset.classes, max_len = 1000\n",
    ")\n",
    "\n",
    "# 각 클래스별로 최대 100개의 샘플을 포함하는 테스트 데이터 서브셋 만듦\n",
    "subset_test_dataset = subset_sampler(\n",
    "    dataset = test_dataset, classes = test_dataset.classes, max_len = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90cf39e2-867c-4d98-a460-44f502021bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "####### 이미지 전처리\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"google/vit-base-patch16-224-in21k\"\n",
    ")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(\n",
    "            size=(\n",
    "                image_processor.size[\"height\"],\n",
    "                image_processor.size[\"width\"]\n",
    "            )\n",
    "        ),\n",
    "        transforms.Lambda(\n",
    "            lambda x: torch.cat([x, x, x], 0)\n",
    "        ),\n",
    "        transforms.Normalize(\n",
    "            mean = image_processor.image_mean,\n",
    "            std  = image_processor.image_std\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb15c67-306b-4ffe-bccd-35bb6fdf2d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 모델 구조에 맞는 형태로 변환 데이터로더 적용\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# swin transformer모델은 Tensor 형식이 아닌 딕셔너리 형식의 데이터를 입력으로 받음\n",
    "# swin transformer모델의 입력: {\"pixel_values\":pixel_values, \"labels\":labels}\n",
    "\n",
    "def collator(data, transform):\n",
    "    images, labels = zip(*data)\n",
    "    # pixel_values = (배치크기, 채널수, 이미지높이, 이미지너비)\n",
    "    pixel_values = torch.stack([transform(image) for image in images])\n",
    "    # labels = [클래스 색인값]\n",
    "    labels       = torch.tensor([label for label in labels])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    subset_train_dataset, # (PIL이미지, label)\n",
    "    batch_size = 32,\n",
    "    shuffle    = True,\n",
    "    collate_fn = lambda x: collator(x, transform),\n",
    "    drop_last  = True\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    subset_test_dataset,\n",
    "    batch_size = 4,\n",
    "    shuffle    = True,\n",
    "    collate_fn = lambda x: collator(x, transform),\n",
    "    drop_last  = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b0e9422-7399-4531-b34d-6d84e1b1d890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swin\n",
      "└ embeddings\n",
      "│  └ patch_embeddings\n",
      "│  │  └ projection Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "│  └ norm\n",
      "│  └ dropout\n",
      "└ encoder\n",
      "│  └ layers\n",
      "│  │  └ 0\n",
      "│  │  └ 1\n",
      "│  │  └ 2\n",
      "│  │  └ 3\n",
      "└ layernorm\n",
      "└ pooler\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "####### 10.13 사전학습된 스윈 트랜스포머 모델\n",
    "from transformers import SwinForImageClassification\n",
    "\n",
    "\n",
    "model = SwinForImageClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"microsoft/swin-tiny-patch4-window7-224\",\n",
    "    # 224*224 이미지, 4*4 크기의 이미지 패치, 최종 7*7개의 로컬 윈도\n",
    "    num_labels                    = len(train_dataset.classes),\n",
    "    id2label                      = {idx: label for label, idx in train_dataset.class_to_idx.items()},\n",
    "    label2id                      = train_dataset.class_to_idx,\n",
    "    ignore_mismatched_sizes       = True\n",
    ")\n",
    "\n",
    "for main_name, main_module in model.named_children():\n",
    "    print(main_name)\n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"└\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"│  └\", ssub_name)\n",
    "            for sssub_name, sssub_module in ssub_module.named_children():\n",
    "                if sssub_name == \"projection\":\n",
    "                    print(\"│  │  └\", sssub_name, sssub_module)\n",
    "                else:\n",
    "                    print(\"│  │  └\", sssub_name)\n",
    "# swin은 cls 토큰 대신 마지막 출력 토큰을 평균하여 사용. 이 평균 작업을 pooler가 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a01960-903b-45f5-9c54-760098f8c211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 차원 : torch.Size([32, 3, 224, 224])\n",
      "모듈: SwinPatchEmbeddings(\n",
      "  (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      ")\n",
      "패치 임베딩 차원 : torch.Size([32, 3136, 96])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "####### 10.14 패치 임베딩 모듈\n",
    "batch = next(iter(train_dataloader))\n",
    "print(\"이미지 차원 :\", batch[\"pixel_values\"].shape)\n",
    "\n",
    "# 패치 임베딩 (224*224*3이미지 - 4*4커널크기, 4간격 -> 56*56(=3136)개 패치(채널=96))\n",
    "patch_emb_output, shape = model.swin.embeddings.patch_embeddings(batch[\"pixel_values\"])\n",
    "print(\"모듈:\", model.swin.embeddings.patch_embeddings)\n",
    "print(\"패치 임베딩 차원 :\", patch_emb_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25955635-41b2-4516-8c5f-7b98fddf9edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks\n",
      "└ 0\n",
      "│ └ layernorm_before\n",
      "│ └ attention\n",
      "│ └ drop_path\n",
      "│ └ layernorm_after\n",
      "│ └ intermediate\n",
      "│ └ output\n",
      "└ 1\n",
      "│ └ layernorm_before\n",
      "│ └ attention\n",
      "│ └ drop_path\n",
      "│ └ layernorm_after\n",
      "│ └ intermediate\n",
      "│ └ output\n",
      "downsample\n",
      "└ reduction\n",
      "└ norm\n"
     ]
    }
   ],
   "source": [
    "####### 10.15 스윈 트랜스포머 블록\n",
    "# stage 1의 스윈 트랜스포머 블록 구조 - blocks(스윈트랜스포머블록), downsample(patch merging)\n",
    "for main_name, main_module in model.swin.encoder.layers[0].named_children():\n",
    "    print(main_name) \n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"└\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"│ └\", ssub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f270eb2-37a6-49ff-b937-4b1bb9d5496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinLayer(\n",
      "  (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  (attention): SwinAttention(\n",
      "    (self): SwinSelfAttention(\n",
      "      (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "      (key): Linear(in_features=96, out_features=96, bias=True)\n",
      "      (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): SwinSelfOutput(\n",
      "      (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (drop_path): SwinDropPath(p=0.1)\n",
      "  (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  (intermediate): SwinIntermediate(\n",
      "    (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): SwinOutput(\n",
      "    (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "####### 10.16 SwinLayer(스윈 트랜스포머 블록) 구조\n",
    "# head가 1개인 경우 (linear에서 채널이 1/head가 되는데 여기선 변화 X)\n",
    "print(model.swin.encoder.layers[0].blocks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a530cd4",
   "metadata": {},
   "source": [
    "- layernorm_before : 첫 번째 계층 정규화\n",
    "- attention : W-MSA(SW-MSA)\n",
    "- layer_norm_after : 두 번째 계층 정규화\n",
    "- intermediate, output : MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d21bf119-4613-4a1c-94c9-0fb002b1989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패치 임베딩 차원 : torch.Size([32, 3136, 96])\n",
      "W-MSA 결과 차원 : torch.Size([32, 3136, 96])\n",
      "SW-MSA 결과 차원 : torch.Size([32, 3136, 96])\n"
     ]
    }
   ],
   "source": [
    "####### 10.17 W-MSA, SW-MSA 모듈\n",
    "print(\"패치 임베딩 차원 :\", patch_emb_output.shape)\n",
    "\n",
    "W_MSA  = model.swin.encoder.layers[0].blocks[0]\n",
    "SW_MSA = model.swin.encoder.layers[0].blocks[1]\n",
    "\n",
    "W_MSA_output  = W_MSA(patch_emb_output, W_MSA.input_resolution)[0]\n",
    "SW_MSA_output = SW_MSA(W_MSA_output, SW_MSA.input_resolution)[0]\n",
    "\n",
    "print(\"W-MSA 결과 차원 :\", W_MSA_output.shape)\n",
    "print(\"SW-MSA 결과 차원 :\", SW_MSA_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e0f0972-23dc-4136-a94c-ec18eb6d3acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_merge 모듈 : SwinPatchMerging(\n",
      "  (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "patch_merge 결과 차원 : torch.Size([32, 784, 192])\n"
     ]
    }
   ],
   "source": [
    "####### 10.18 패치 병합 - 패치를 절반 크기로 분할하는 reduction, 계층 정규화\n",
    "# 1. 2*2 그룹으로 나눠 채널기준으로 병합 : 32*3136(56*56)*96 -> 32*784(28*28)*384(96*4)\n",
    "# 2. 차원을 절반으로 축소 : 32*784*384 -> 32*784*192\n",
    "patch_merge = model.swin.encoder.layers[0].downsample\n",
    "print(\"patch_merge 모듈 :\", patch_merge)\n",
    "\n",
    "# patch_merge로 해상도 1/4배, 채널 2배\n",
    "output = patch_merge(SW_MSA_output, patch_merge.input_resolution)\n",
    "print(\"patch_merge 결과 차원 :\", output.shape)\n",
    "\n",
    "# 이렇게 마지막 스테이지4까지 통과하면 [N, 49(7*7), 768]이 됨\n",
    "# 이 특징맵이 풀링 계층을 통과해 클래스를 예측함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
