{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e00d6d-5009-4113-b19c-a6aae119bd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "def subset_sampler(dataset, classes, max_len):\n",
    "    target_idx = defaultdict(list)\n",
    "    for idx, label in enumerate(dataset.train_labels):\n",
    "        target_idx[int(label)].append(idx)\n",
    "\n",
    "    indices = list(\n",
    "        chain.from_iterable(\n",
    "            [target_idx[idx][:max_len] for idx in range(len(classes))]\n",
    "        )\n",
    "    )\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=True)\n",
    "test_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=False)\n",
    "\n",
    "classes = train_dataset.classes\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "\n",
    "subset_train_dataset = subset_sampler(\n",
    "    dataset=train_dataset, classes=train_dataset.classes, max_len=1000\n",
    ")\n",
    "subset_test_dataset = subset_sampler(\n",
    "    dataset=test_dataset, classes=test_dataset.classes, max_len=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "787380b7-4525-4a00-b77e-84c9c568cd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d46a6c07aa4d4bba0d73f992a7d544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/266 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"microsoft/cvt-21\"\n",
    ")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(\n",
    "            size=(\n",
    "                image_processor.size[\"shortest_edge\"],\n",
    "                image_processor.size[\"shortest_edge\"]\n",
    "            )\n",
    "        ),\n",
    "        transforms.Lambda(lambda x: torch.cat([x, x, x], 0)),\n",
    "        transforms.Normalize(\n",
    "            mean=image_processor.image_mean,\n",
    "            std=image_processor.image_std\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cabbd62a-5ea5-4738-b631-7ef97dfd50d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collator(data, transform):\n",
    "    images, labels = zip(*data)\n",
    "    pixel_values = torch.stack([transform(image) for image in images])\n",
    "    labels = torch.tensor([label for label in labels])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    subset_train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: collator(x, transform),\n",
    "    drop_last=True\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    subset_test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: collator(x, transform),\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d0f969a-c315-4bfc-a2e3-ac64ce375606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b93076cfd67431dadc7f2e888887b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/70.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22568c58d6245fda4dced34263119ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/127M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CvtForImageClassification were not initialized from the model checkpoint at microsoft/cvt-21 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 384]) in the checkpoint and torch.Size([10, 384]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvt\n",
      "└ encoder\n",
      "   └ stages\n",
      "     └ 0\n",
      "     └ 1\n",
      "     └ 2\n",
      "layernorm\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "from transformers import CvtForImageClassification\n",
    "\n",
    "\n",
    "model = CvtForImageClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"microsoft/cvt-21\",\n",
    "    num_labels=len(train_dataset.classes),\n",
    "    id2label={idx: label for label, idx in train_dataset.class_to_idx.items()},\n",
    "    label2id=train_dataset.class_to_idx,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "for main_name, main_module in model.named_children():\n",
    "    print(main_name)\n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"└\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"   └\", ssub_name)\n",
    "            for sssub_name, sssub_module in ssub_module.named_children():\n",
    "                print(\"     └\", sssub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669cab46-cf7b-4578-b14f-ab73ecb70de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CvtStage(\n",
      "  (embedding): CvtEmbeddings(\n",
      "    (convolution_embeddings): CvtConvEmbeddings(\n",
      "      (projection): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
      "      (normalization): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): CvtLayer(\n",
      "      (attention): CvtAttention(\n",
      "        (attention): CvtSelfAttention(\n",
      "          (convolution_projection_query): CvtSelfAttentionProjection(\n",
      "            (convolution_projection): CvtSelfAttentionConvProjection(\n",
      "              (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "              (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (linear_projection): CvtSelfAttentionLinearProjection()\n",
      "          )\n",
      "          (convolution_projection_key): CvtSelfAttentionProjection(\n",
      "            (convolution_projection): CvtSelfAttentionConvProjection(\n",
      "              (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "              (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (linear_projection): CvtSelfAttentionLinearProjection()\n",
      "          )\n",
      "          (convolution_projection_value): CvtSelfAttentionProjection(\n",
      "            (convolution_projection): CvtSelfAttentionConvProjection(\n",
      "              (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "              (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (linear_projection): CvtSelfAttentionLinearProjection()\n",
      "          )\n",
      "          (projection_query): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (projection_key): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (projection_value): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (output): CvtSelfOutput(\n",
      "          (dense): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): CvtIntermediate(\n",
      "        (dense): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (output): CvtOutput(\n",
      "        (dense): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (layernorm_before): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (layernorm_after): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "stages = model.cvt.encoder.stages\n",
    "print(stages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa8a350-6ba4-4508-ab95-1144aa64b748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 차원 : torch.Size([32, 3, 224, 224])\n",
      "패치 임베딩 차원 : torch.Size([32, 64, 56, 56])\n",
      "셀프 어텐션 입력 차원 : torch.Size([32, 3136, 64])\n",
      "셀프 어텐션 출력 차원 : torch.Size([32, 3136, 64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(\"이미지 차원 :\", batch[\"pixel_values\"].shape)\n",
    "\n",
    "patch_emb_output = stages[0].embedding(batch[\"pixel_values\"])\n",
    "print(\"패치 임베딩 차원 :\", patch_emb_output.shape)\n",
    "\n",
    "batch_size, num_channels, height, width = patch_emb_output.shape\n",
    "hidden_state = patch_emb_output.view(batch_size, num_channels, height * width).permute(0, 2, 1)\n",
    "print(\"셀프 어텐션 입력 차원 :\", hidden_state.shape)\n",
    "\n",
    "attention_output = stages[0].layers[0].attention.attention(hidden_state, height, width)\n",
    "print(\"셀프 어텐션 출력 차원 :\", attention_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa0225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
