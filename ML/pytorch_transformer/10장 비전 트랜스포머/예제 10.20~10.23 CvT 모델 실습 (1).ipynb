{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e00d6d-5009-4113-b19c-a6aae119bd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "####### 데이터셋 불러오고 서브셋 만들기 (vit, swin과 동일)\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "def subset_sampler(dataset, classes, max_len): \n",
    "    target_idx = defaultdict(list)\n",
    "    for idx, label in enumerate(dataset.train_labels): \n",
    "        target_idx[int(label)].append(idx)\n",
    "\n",
    "    indices = list(\n",
    "        chain.from_iterable(\n",
    "            [target_idx[idx][:max_len] for idx in range(len(classes))]\n",
    "        )\n",
    "    )\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=True)\n",
    "test_dataset  = datasets.FashionMNIST(root=\"../datasets\", download=True, train=False)\n",
    "\n",
    "classes      = train_dataset.classes\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "\n",
    "subset_train_dataset = subset_sampler(\n",
    "    dataset = train_dataset, classes = train_dataset.classes, max_len = 1000\n",
    ")\n",
    "subset_test_dataset = subset_sampler(\n",
    "    dataset = test_dataset, classes = test_dataset.classes, max_len = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "787380b7-4525-4a00-b77e-84c9c568cd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "####### 10.20 CvT모델 이미지 데이터 전처리\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"microsoft/cvt-21\"\n",
    ")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # 모델 학습을 위해 PIL.image > Tensor\n",
    "        transforms.ToTensor(),\n",
    "        # 크기 조정\n",
    "        transforms.Resize(\n",
    "            size=(\n",
    "                # 기존에는 224*224 강제조정이라면 여기선 비율유지\n",
    "                # 예를 들어, 원본 이미지의 크기가 800x600(가로x세로)일 때, shortest_edge를 224로 설정하면 이미지의 세로 길이가 224이 되고, 가로 길이는 원본 비율을 유지하여 224 * (800/600) = 299이 된다. 결과적으로 이미지의 크기는 299x224이 된다.\n",
    "                image_processor.size[\"shortest_edge\"],  # shortest_edge: 이미지의 너비나 높이 중 더 작은 값 (224)\n",
    "                image_processor.size[\"shortest_edge\"]\n",
    "            )\n",
    "        ),\n",
    "        # 단일 채널을 복제해 다중 채널 이미지로 변환\n",
    "            # FashinMNIST의 x는 흑백이미지 [1,H,W]\n",
    "            # 텐서 3개로 복사하여 0번차원 기준으로 cat(연결)\n",
    "            #결과: [3,H,W]\n",
    "        transforms.Lambda(lambda x: torch.cat([x, x, x], 0)),\n",
    "        # 정규화\n",
    "        transforms.Normalize(\n",
    "            mean = image_processor.image_mean,\n",
    "            std  = image_processor.image_std\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cabbd62a-5ea5-4738-b631-7ef97dfd50d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 모델 구조에 맞는 형태로 변환 데이터로더 적용\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Tensor 형식이 아닌 딕셔너리 형식의 데이터를 입력으로 받음\n",
    "# 모델의 입력: {\"pixel_values\":pixel_values, \"labels\":labels}\n",
    "def collator(data, transform):\n",
    "    images, labels = zip(*data)\n",
    "    # pixel_values = (배치크기, 채널수, 이미지높이, 이미지너비)\n",
    "    pixel_values = torch.stack([transform(image) for image in images])\n",
    "    # labels = [클래스 색인값]\n",
    "    labels       = torch.tensor([label for label in labels])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    subset_train_dataset,\n",
    "    batch_size = 32,\n",
    "    shuffle    = True,\n",
    "    collate_fn = lambda x: collator(x, transform),\n",
    "    drop_last  = True\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    subset_test_dataset,\n",
    "    batch_size = 4,\n",
    "    shuffle    = True,\n",
    "    collate_fn = lambda x: collator(x, transform),\n",
    "    drop_last  = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0f969a-c315-4bfc-a2e3-ac64ce375606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of CvtForImageClassification were not initialized from the model checkpoint at microsoft/cvt-21 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 384]) in the checkpoint and torch.Size([10, 384]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvt\n",
      "└ encoder\n",
      "   └ stages\n",
      "     └ 0\n",
      "     └ 1\n",
      "     └ 2\n",
      "layernorm\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "####### 10.21 사전 학습된 CvT 모델\n",
    "from transformers import CvtForImageClassification\n",
    "\n",
    "\n",
    "model = CvtForImageClassification.from_pretrained(\n",
    "    # 사전 학습된 모델은 앞의 이미지 프로세서 클래스에서 사용한 모델과 동일\n",
    "    pretrained_model_name_or_path = \"microsoft/cvt-21\",\n",
    "    # 아래의 3개 매개변수를 통해 현재 데이터세트에 적합한 구조로 모델을 미세조정\n",
    "    num_labels                    = len(train_dataset.classes),\n",
    "    id2label                      = {idx: label for label, idx in train_dataset.class_to_idx.items()},\n",
    "    label2id                      = train_dataset.class_to_idx,\n",
    "    ignore_mismatched_sizes       = True\n",
    ")\n",
    "\n",
    "for main_name, main_module in model.named_children(): \n",
    "    print(main_name)\n",
    "    for sub_name, sub_module in main_module.named_children(): \n",
    "        print(\"└\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children(): \n",
    "            print(\"   └\", ssub_name)\n",
    "            for sssub_name, sssub_module in ssub_module.named_children(): \n",
    "                print(\"     └\", sssub_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c3f9b",
   "metadata": {},
   "source": [
    "스윈 트랜스포머와의 차이점  \n",
    "1. classifier 이전에 pooler 계층 존재 X\n",
    "2. 스테이지별 이미지 크기와 임베딩 차원\n",
    "    - 이미지 사이즈는 스테이지별로 4배, 8배, 16배 감소됨\n",
    "    - 이미지 사이즈와 반비례하게 임베딩 차원이 커짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "669cab46-cf7b-4578-b14f-ab73ecb70de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CvtStage(\n",
      "  (embedding): CvtEmbeddings(\n",
      "    (convolution_embeddings): CvtConvEmbeddings(\n",
      "      (projection): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
      "      (normalization): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): CvtLayer(\n",
      "      (attention): CvtAttention(\n",
      "        (attention): CvtSelfAttention(\n",
      "          (convolution_projection_query): CvtSelfAttentionProjection(\n",
      "            (convolution_projection): CvtSelfAttentionConvProjection(\n",
      "              (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "              (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (linear_projection): CvtSelfAttentionLinearProjection()\n",
      "          )\n",
      "          (convolution_projection_key): CvtSelfAttentionProjection(\n",
      "            (convolution_projection): CvtSelfAttentionConvProjection(\n",
      "              (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "              (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (linear_projection): CvtSelfAttentionLinearProjection()\n",
      "          )\n",
      "          (convolution_projection_value): CvtSelfAttentionProjection(\n",
      "            (convolution_projection): CvtSelfAttentionConvProjection(\n",
      "              (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "              (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (linear_projection): CvtSelfAttentionLinearProjection()\n",
      "          )\n",
      "          (projection_query): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (projection_key): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (projection_value): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (output): CvtSelfOutput(\n",
      "          (dense): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): CvtIntermediate(\n",
      "        (dense): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (output): CvtOutput(\n",
      "        (dense): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (layernorm_before): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (layernorm_after): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCvT Embedding\\n- 224*224 > 56*56(소수점버림) => [N,64,56,56]\\nCvT Layer\\n- 매개변수가 축소된 어텐션 임베딩 계산 위해 key, value 임베딩 간격을 1보다 큰 2로 설정해 차원 축소\\n- 3136(56*56)개의 이미지 패치에 대한 셀프 어텐션 수행\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### 10.22 CvT 모델의 스테이지 구조\n",
    "# CvT Stage = CvT Embeddings(패치 단위로 잘라 임베딩) + CvT Layer()\n",
    "stages = model.cvt.encoder.stages\n",
    "print(stages[0])\n",
    "\n",
    "'''\n",
    "CvT Embedding\n",
    "- 224*224 > 56*56(소수점버림) => [N,64,56,56]\n",
    "CvT Layer\n",
    "- 매개변수가 축소된 어텐션 임베딩 계산 위해 key, value 임베딩 간격을 1보다 큰 2로 설정해 차원 축소\n",
    "- 3136(56*56)개의 이미지 패치에 대한 셀프 어텐션 수행\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aa8a350-6ba4-4508-ab95-1144aa64b748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 차원 : torch.Size([32, 3, 224, 224])\n",
      "패치 임베딩 차원 : torch.Size([32, 64, 56, 56])\n",
      "셀프 어텐션 입력 차원 : torch.Size([32, 3136, 64])\n",
      "셀프 어텐션 출력 차원 : torch.Size([32, 3136, 64])\n"
     ]
    }
   ],
   "source": [
    "####### 10.23 셀프 어텐션 적용\n",
    "batch = next(iter(train_dataloader))\n",
    "print(\"이미지 차원 :\", batch[\"pixel_values\"].shape)\n",
    "\n",
    "patch_emb_output = stages[0].embedding(batch[\"pixel_values\"])\n",
    "print(\"패치 임베딩 차원 :\", patch_emb_output.shape)\n",
    "\n",
    "batch_size, num_channels, height, width = patch_emb_output.shape\n",
    "hidden_state = patch_emb_output.view(batch_size, num_channels, height * width).permute(0, 2, 1)\n",
    "print(\"셀프 어텐션 입력 차원 :\", hidden_state.shape)\n",
    "\n",
    "# self-attention 수행 (입력 차원과 출력 차원 형태 동일)\n",
    "attention_output = stages[0].layers[0].attention.attention(hidden_state, height, width)\n",
    "print(\"셀프 어텐션 출력 차원 :\", attention_output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
